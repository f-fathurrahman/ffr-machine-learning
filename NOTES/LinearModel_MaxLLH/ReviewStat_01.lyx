#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{babel}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding iso8859-15
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family sfdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Random variables and probability
\end_layout

\begin_layout Standard
Random variables
\end_layout

\begin_layout Standard
Random variables can be described as variables whose values depend on outcomes
 of random events.
 There are two kinds of random variables: discrete and continuous random
 variables.
 Discrete random variables are used for random events for which we can systemati
cally list (or count) all possible outcomes.
 Continuous random variables are used when we can not list all possible
 outcomes.
 It is a common convention to use upper-case letters to describe random
 variables and lower-case ones for possible values that the random variable
 can take.
\end_layout

\begin_layout Standard
A discrete random variable could be used to describe, for examples:
\end_layout

\begin_layout Itemize
a coin toss: possible outcomes are head or tail.
\end_layout

\begin_layout Itemize
rolling of a die: possible outcomes are 1,2,3,4,5 or 6.
\end_layout

\begin_layout Standard
A continous random variable could be used to describe, for examples:
\end_layout

\begin_layout Itemize
outcome of a 100m race
\end_layout

\begin_layout Itemize
height of a human
\end_layout

\begin_layout Itemize
mass of a pebble
\end_layout

\begin_layout Standard
The collection of possible outcomes is known as the sample space.
\end_layout

\begin_layout Standard
Probability and distributions
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Y$
\end_inset

 be a random variable that represents the toss of a coin.
 If the coin lands heads, 
\begin_inset Formula $Y=1$
\end_inset

 and if tails, 
\begin_inset Formula $Y=0$
\end_inset

.
 To model this event (the coin toss), we need to be able to quantify how
 likely either outcome is.
 For discrete random variables, we do this by defining 
\emph on
probabilities
\emph default
 of different outcomes.
\end_layout

\begin_layout Standard
Two important rules governing probabilities:
\end_layout

\begin_layout Itemize
Probabilities must be greater than or equal to 0 and less than of equal
 to 1:
\begin_inset Formula 
\[
0\leq P(Y=y)\leq1
\]

\end_inset


\end_layout

\begin_layout Itemize
The sum of the probabilities of each possible individual outcome must be
 equal to 1:
\begin_inset Formula 
\[
\sum_{y}P(Y=y)=1
\]

\end_inset


\end_layout

\begin_layout Standard
For a coin we have:
\begin_inset Formula 
\[
P(Y=1)+P(Y=0)=1
\]

\end_inset

For a fair coin we have:
\begin_inset Formula 
\[
P(Y=1)=P(Y=0)=0.5
\]

\end_inset

We will sometimes use the following shorthand
\begin_inset Formula 
\[
P(Y=y)=P(y)
\]

\end_inset


\end_layout

\begin_layout Standard
The set of all possible outcomes (all of the 
\begin_inset Formula $y$
\end_inset

s) and their probabilities, 
\begin_inset Formula $P(y)$
\end_inset

, is known as probability distribution.
 It tell us how the total probability is distributed over all possible outcomes.
\end_layout

\begin_layout Standard
Adding probabilities
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Y$
\end_inset

 be a random variable for modeling outcome of rolling of a fair die.
 For exmple we want to know the probability of the result being lower than
 4.
 The outcomes that are lower than 4 are 1, 2, and 3, which means that we
 need to be able to calculate the probability that the die lands 1 or 2
 or 3.
 We can calculate this by using the additive law of probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(Y<4)=P(Y=1)+P(Y=2)+P(Y=3)
\]

\end_inset


\end_layout

\begin_layout Standard
Conditional probabilities
\end_layout

\begin_layout Standard
Often one event will affect the outcome of another.
 We can use conditional probabilities to express the probability that 
\begin_inset Formula $Y$
\end_inset

 takes a particular value given that 
\begin_inset Formula $X$
\end_inset

 has taken a particular value.
 This can be expressed as 
\begin_inset Formula $P(Y=y|X=x)$
\end_inset

 or using shorthand notation $P(y|x)$.
 This notation also can be read as the probability that 
\begin_inset Formula $Y$
\end_inset

 has the outcome of 
\begin_inset Formula $y$
\end_inset

 given that 
\begin_inset Formula $X$
\end_inset

 has the outcome 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
Joint probabilities
\end_layout

\begin_layout Standard
Given two (or more) random variables, we want to know the probability that
 they each take a particular value.
 For example the probability that 
\begin_inset Formula $Y$
\end_inset

 has the outcome 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 has the outcome 
\begin_inset Formula $x$
\end_inset

 is written as
\begin_inset Formula 
\[
P(Y=y,X=y)
\]

\end_inset

or in shorthand notation 
\begin_inset Formula $p(x,y)$
\end_inset

.
 How we deal with these joint distributions depends on whether or not the
 random variables are dependent.
 If the events are independent we have:
\begin_inset Formula 
\[
P(y_{1},y_{2},\ldots,y_{j})=\prod_{j=1}^{J}P(y_{j})
\]

\end_inset


\end_layout

\begin_layout Standard
It the events are dependent, we cannot decompose the joint probability as
 products of individual probabilities.
 However, if we can create conditional distributions, we can decompose the
 joint probability using the following definitions
\begin_inset Formula 
\[
P(Y=y,X=x)=P(Y=y|X=x)P(X=x)
\]

\end_inset


\end_layout

\begin_layout Standard
or 
\begin_inset Formula 
\[
P(Y=y,X=x)=P(X=x|Y=y)P(Y=y)
\]

\end_inset


\end_layout

\begin_layout Standard
Marginalization
\end_layout

\begin_layout Standard
Given joint probability 
\begin_inset Formula $P(Y=y,X=x)$
\end_inset

 we can obtain 
\begin_inset Formula $P(Y=y)$
\end_inset

 by marginalizing out 
\begin_inset Formula $X$
\end_inset

 from the joint distribution.
 This can be done by summing the joint probabilities over all possible values
 of 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\[
P(Y=y)=\sum_{x}P(Y=y,X=x)
\]

\end_inset

More generally we have
\begin_inset Formula 
\[
P(Y_{j}=y_{j})=P(y_{j})=\sum_{y_{1},\ldots,y_{j-1},y_{j+1},\ldots,y_{J}}P(y_{1},y_{2},\ldots,y_{J})
\]

\end_inset


\end_layout

\begin_layout Standard
Bayes' rule
\end_layout

\begin_layout Standard
Bayes' rule can be used for reversing conditioning of probability:
\begin_inset Formula 
\[
P(x|y)=\frac{P(y|x)P(x)}{P(y)}
\]

\end_inset


\end_layout

\begin_layout Standard
Expectations
\end_layout

\begin_layout Standard
An expectation tells us what value we would expect some function 
\begin_inset Formula $f(X)$
\end_inset

 of a random variable 
\begin_inset Formula $X$
\end_inset

 to take and is defined (for discrete random variables) as
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{f(X)\}=\sum_{x}f(x)\,P(x)
\]

\end_inset

A common expectation that we will encounter is the mean, which is the expectatio
n of 
\begin_inset Formula $f(X)=X$
\end_inset

:
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{X\}=\sum_{x}x\,P(x)
\]

\end_inset

For a fair die, for example, 
\begin_inset Formula $P(x)=\dfrac{1}{6}$
\end_inset

, we have:
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{X\}=\sum_{x}x\frac{1}{6}=\frac{1}{6}+\frac{2}{6}+\ldots+\frac{6}{6}=\frac{21}{6}=3.5
\]

\end_inset


\end_layout

\begin_layout Standard
Another example, for a die, the expected value of 
\begin_inset Formula $f(X)=X^{2}$
\end_inset

 is:
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{X^{2}\}=\sum_{x}x^{2}\frac{1}{6}=\frac{1}{6}+\frac{4}{6}+\cdots+\frac{36}{6}=\frac{91}{6}
\]

\end_inset

The expected value of a function 
\begin_inset Formula $X$
\end_inset

 is not in general the function evaluated at the expected value of 
\begin_inset Formula $X$
\end_inset

.
 Mathematically
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{f(X)\}\neq f(\mathbf{E}_{P(x)}\{X\}),\,\,\text{(generally)}
\]

\end_inset


\end_layout

\begin_layout Standard
One situation where the two are equal is when the function is just a constant
 multiplied by 
\begin_inset Formula $X$
\end_inset

.
 For example, 
\begin_inset Formula $f(X)=aX$
\end_inset


\begin_inset Formula 
\[
\begin{align*}\mathbf{E}_{P(x)}\{f(X)\} & =\sum_{x}axP(x)\\
 & =a\sum_{x}xP(x)\\
 & =a\mathbf{E}_{P(x)}\{X\}\\
 & =f\left(\mathbf{E}_{P(x)}\{X\}\right)
\end{align*}
\]

\end_inset


\end_layout

\begin_layout Standard
Another important case is when the function is simply a constant, 
\begin_inset Formula $f(X)=a$
\end_inset

.
 In this case, the expectation disappears due to the fact that the distribution
 has to sum to 1 over all possible outcomes:
\begin_inset Formula 
\[
\begin{align*}\mathbf{E}_{P(x)}\{f(X)\} & =\sum_{x}aP(x)\\
 & =a\sum_{x}P(x)\\
 & =a
\end{align*}
\]

\end_inset


\end_layout

\begin_layout Standard
Expectation of a sum of different functions is equal to a sum of the individual
 expectations:
\begin_inset Formula 
\[
\begin{align*}\mathbf{E}_{P(x)}\{f(X)+g(X)\} & =\sum_{x}(f(x)+g(x))P(x)\\
 & =\sum_{x}f(x)P(x)+\sum_{x}g(x)P(x)\\
 & =\mathbf{E}_{P(x)}\{f(X)\}+\mathbf{E}_{P(x)}\{g(X)\}
\end{align*}
\]

\end_inset


\end_layout

\begin_layout Standard
Besides mean, another common expectation value that we will encounter is
 the variance.
 Variance is a measure of how variable the random variable is and is defined
 as the expected square deviation from the mean.
\begin_inset Formula 
\[
\mathrm{var}\{X\}=\mathbf{E}_{P(x)}\left\{ \left(X-\mathbf{E}_{P(x)}\{X\}\right)^{2}\right\} 
\]

\end_inset

Expand the terms in bracket:
\begin_inset Formula 
\[
\begin{align*}\mathrm{var}\{X\} & =\mathbf{E}_{P(x)}\left\{ X^{2}-2X\mathbf{E}_{P(x)}\{X\}+\mathbf{E}_{P(x)}\{X\}^{2}\right\} \\
 & =\mathbf{E}_{P(x)}\{X^{2}\}-2\mathbf{E}_{P(x)}\{X\}\mathbf{E}_{P(x)}\{X\}+\mathbf{E}_{P(x)}\{X\}^{2}
\end{align*}
\]

\end_inset

Collecting together the 
\begin_inset Formula $\mathbf{E}_{P(x)}\{X\}^{2}$
\end_inset

 terms gives
\begin_inset Formula 
\[
\mathrm{var}\{X\}=\mathbf{E}_{P(x)}\{X^{2}\}-\mathbf{E}_{P(x)}\{X\}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\backslash
textbf{Vector random variables}
\end_layout

\begin_layout Standard
Probability distributions over vectors is nothing more than a shorthand
 way of defining large joint distributions.
 For example the values that could be taken on by random variables 
\begin_inset Formula $X_{1},X_{2},\ldots,X_{N}$
\end_inset

 can be expressed as the vector 
\begin_inset Formula $\mathbf{x}=[x_{1},x_{2},\ldots,x_{N}]^{\mathsf{T}}$
\end_inset

.
 Using this shorthand notation:
\begin_inset Formula 
\[
p(\mathbf{x})=p(x_{1},x_{2},\ldots,x_{N})=P(X_{1}=x_{1},X_{2}=x_{2},\ldots,X_{N}=x_{N})
\]

\end_inset

Even though 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is a vector, 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 is a scalar quantity.
\end_layout

\begin_layout Standard
Expectations are computed for vector random variables in the same way.
\begin_inset Formula 
\[
\mathbf{E}_{P(x)}\{f(\mathbf{x})\}=\sum_{\mathbf{x}}f(\mathbf{x})P(\mathbf{x})
\]

\end_inset

where the sum is over all possible values of the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Standard
The mean vector is defined as:
\begin_inset Formula 
\[
\mathbf{E}_{P(\mathbf{x})}=\sum_{\mathbf{x}}\mathbf{x}\,P(\mathbf{x})
\]

\end_inset


\end_layout

\begin_layout Standard
When dealing with vectors, the concept of variance is generalized to a 
\emph on
covariance matrix
\emph default
.
 This is defined as
\begin_inset Formula 
\[
\mathrm{cov}\{\mathbf{x}\}=\mathbf{E}_{P(\mathbf{x})}\left\{ \left(\mathbf{x}-\mathbf{E}_{P(\mathbf{x})}{\mathbf{x}}\right)\left(\mathbf{x}-\mathbf{E}_{P(\mathbf{x})}{\mathbf{x}}\right)^{\mathsf{T}}\right\} 
\]

\end_inset

If 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is a vector of length 
\begin_inset Formula $D$
\end_inset

, then 
\begin_inset Formula $\mathrm{cov}\{\mathbf{x}\}$
\end_inset

 is a 
\begin_inset Formula $D\times D$
\end_inset

 matrix.
 The diagonal elements correspond to the variance of the individual elements
 of 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Standard
The off-diagonal elements tell us to what extent different elements of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 co-vary, that is, how dependent they are on one another.
\end_layout

\begin_layout Standard
A high positive value between, say, elements 
\begin_inset Formula $x_{d}$
\end_inset

 and 
\begin_inset Formula $x_{e}$
\end_inset

 suggest that if 
\begin_inset Formula $x_{d}$
\end_inset

 increases, so does 
\begin_inset Formula $x_{e}$
\end_inset

.
 A high negative value suggests that they are related but move in opposite
 directions.
 A value of or close to zero suggest that there is no relationship between
 them.
\end_layout

\begin_layout Standard
The covariance for vector random variables also can be written as
\begin_inset Formula 
\[
\begin{align*}\mathrm{cov}\{\mathbf{x}\} & =\mathbf{E}_{P(\mathbf{x})}\left\{ \left(\mathbf{x}-\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}\right)\left(\mathbf{x}-\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}\right)^{\mathsf{T}}\right\} \\
 & =\mathbf{E}_{P(\mathbf{x})}\left\{ \mathbf{xx}^{\mathsf{T}}-2\mathbf{x}\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}^{\mathsf{T}}-\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}^{\mathsf{T}}\right\} 
\end{align*}
\]

\end_inset

We finally obtain:
\begin_inset Formula 
\[
\mathrm{cov}\{\mathbf{x}\}=\mathbf{E}_{P(\mathbf{x})}\left\{ \mathbf{x}\mathbf{x}^{\mathsf{T}}\right\} -\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}\}\mathbf{E}_{P(\mathbf{x})}\{\mathbf{x}^{\mathsf{T}}\}
\]

\end_inset


\end_layout

\begin_layout Standard
Popular discrete distributions
\end_layout

\begin_layout Standard
Bernoulli distribution
\end_layout

\begin_layout Standard
For a random variable 
\begin_inset Formula $X$
\end_inset

 that can take two values, 0 or 1 (a binary random variable), where the
 probability that it takes the value 1 is defined as 
\begin_inset Formula $q$
\end_inset

, the Bernoulli distribution is
\begin_inset Formula 
\[
P(X=x)=q^{x}(1-q)^{1-x}
\]

\end_inset


\end_layout

\begin_layout Standard
Binomial distribution
\end_layout

\begin_layout Standard
The binomial distribution extends the Bernoulli distribution to define the
 probability of observing a certain number of heads in a total of 
\begin_inset Formula $N$
\end_inset

 tosses.
 More generally, we might think of events that have two outcomes (success
 or failure).
 If we have 
\begin_inset Formula $N$
\end_inset

 such events, the binomial random variable 
\begin_inset Formula $Y$
\end_inset

 can take the values from 0 (no success) to 
\begin_inset Formula $N$
\end_inset

 (
\begin_inset Formula $N$
\end_inset

 successes).
 The probability of observing a particular number of successes is given
 by:
\begin_inset Formula 
\[
P(Y=y)=P(y)=\begin{pmatrix}N\\
y
\end{pmatrix}q^{y}(1-q)^{N-y}
\]

\end_inset

where:
\begin_inset Formula 
\[
\begin{pmatrix}N\\
y
\end{pmatrix}=\frac{N!}{y!(N-y)!}
\]

\end_inset

is a mathematical shorthand for the number of ways in which 
\begin_inset Formula $y$
\end_inset

 distinct objects can be chosen from a set of 
\begin_inset Formula $N$
\end_inset

 objects.
\end_layout

\begin_layout Standard
The Bernoulli distribution is a special case of the binomial distribution
 when 
\begin_inset Formula $N=1$
\end_inset

.
\end_layout

\begin_layout Standard
Multinomial distribution
\end_layout

\begin_layout Standard
Multinomial distribution is a generalization of binomial distribution to
 vector random variables.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(Y=\mathbf{y})=P(\mathbf{y})=\frac{N!}{\prod_{j}y_{j}!}\prod_{j}q_{j}^{y_{j}}
\]

\end_inset


\begin_inset Formula $q_{j}$
\end_inset

 are the parameters of the multinomial distribution:
\begin_inset Formula 
\[
\sum_{j}q_{j}=1
\]

\end_inset


\end_layout

\begin_layout Standard
Continuous random variables - density functions
\end_layout

\begin_layout Standard
When working with continuous random variables, we need a continuous analogue
 to the probability distribution.
 This is provided by a 
\emph on
probability density function
\emph default
 (pdf) or simply 
\emph on
density
\emph default
, also denoted by 
\begin_inset Formula $p(x)$
\end_inset

.
 To compute the probability that 
\begin_inset Formula $X$
\end_inset

 lies in a particular range, we compute the definite integral of 
\begin_inset Formula $p(x)$
\end_inset

 with respect to 
\begin_inset Formula $x$
\end_inset

 over this range:
\begin_inset Formula 
\[
P(x_{1}\leq X\leq x_{2})=\int_{x_{1}}^{x_{2}}p(x)\,\mathrm{d}x
\]

\end_inset


\end_layout

\begin_layout Standard
If a random variable 
\begin_inset Formula $X$
\end_inset

 only takes values in the range 
\begin_inset Formula $x_{1}\leq X\leq x_{2}$
\end_inset

 we have:
\begin_inset Formula 
\[
\int_{x_{1}}^{x_{2}}p(x)\,\mathrm{d}x=1,\,\,\text{where }x_{1}\leq X\leq x_{2}
\]

\end_inset

We also have:
\begin_inset Formula 
\[
p(x)\geq0
\]

\end_inset


\end_layout

\begin_layout Standard
Note that there is no upper bound on the value of pdf because it is not
 a probability and can be higher than 1 for a particular value of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
We also can define joint pdf over several continuous random variables.
 For example 
\begin_inset Formula $p(x,y)$
\end_inset

 the joint density of two random variables 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $p(\mathbf{w})$
\end_inset

 is the density of a vector 
\begin_inset Formula $\mathbf{w}$
\end_inset

 which could be thought of as the joint density of 
\begin_inset Formula $p(w_{0},w_{1},\ldots)$
\end_inset

 random variables representing each element in the vector.
 Although we cannot compute 
\begin_inset Formula $P(X=x,Y=y)$
\end_inset

 we can compute
\begin_inset Formula 
\[
P(x_{1}\leq X\leq x_{2},y_{1}\leq Y\leq y_{2})=\int_{x=x_{1}}^{x=x_{2}}\int_{y=y_{1}}^{y=y_{2}}p(x,y)\,\mathrm{d}x\,\mathrm{d}y
\]

\end_inset


\end_layout

\begin_layout Standard
The same applies for conditional distributions, although the conditioning
 is done on exact value (as this event is assume to have happened or known).
 For example we can compute:
\begin_inset Formula 
\[
P(x_{1}\leq X\leq x_{2}|Y=y)=\int_{x=x_{1}}^{x=x_{2}}p(x|Y=y)\,\mathrm{d}x
\]

\end_inset


\end_layout

\begin_layout Standard
For marginalization, we can use integration instead of summation.
 For example the pdf 
\begin_inset Formula $p(y)$
\end_inset

 can be computed from 
\begin_inset Formula $p(y,x)$
\end_inset

 as follows:
\begin_inset Formula 
\[
p(y)=\int_{x=x_{1}}^{x=x_{2}}p(y,x)\,\mathrm{d}x
\]

\end_inset

where 
\begin_inset Formula $x_{1}\leq X\leq x_{2}$
\end_inset

 described the sample space of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
Expectations with respect to continous random variables are performed by
 integrating over the range of values that the random variable can take:
\begin_inset Formula 
\[
\mathbf{E}_{p(x)}\{f(x)\}=\int f(x)p(x)\,\mathrm{d}x
\]

\end_inset

For many cases, this integration cannot be performed analytically.
 In this case, we can approximate it using simple summation:
\begin_inset Formula 
\[
\mathbf{E}_{p(x)}\{f(x)\}\approx\frac{1}{S}\sum_{s=1}^{S}f(x_{s})P(x_{s})
\]

\end_inset


\end_layout

\begin_layout Standard
Popular continuous density function
\end_layout

\begin_layout Standard
Uniform density function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y)=\begin{cases}
r & \text{for }a\leq y\leq b\\
0 & \text{otherwise}
\end{cases}
\]

\end_inset

Given 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

, the value of 
\begin_inset Formula $r$
\end_inset

 can be calculated from the condition:
\begin_inset Formula 
\[
P(a\leq Y\leq b)=\int_{y=a}^{y=b}p(y)\,\mathrm{d}y=1
\]

\end_inset

So we have
\begin_inset Formula 
\[
r=\frac{1}{b-a}
\]

\end_inset


\end_layout

\begin_layout Standard
Beta density function
\end_layout

\begin_layout Standard
Beta density function can be used for continuous random variables that are
 restricted to between 0 and 1.
 The beta density function is defined as:
\begin_inset Formula 
\[
p(r)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}r^{\alpha-1}(1-r)^{\beta-1}
\]

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are positive parameters that control the shape of the density function.
 
\begin_inset Formula $\Gamma(z)$
\end_inset

 is known as the gamma function.
\end_layout

\begin_layout Standard
Gaussian or normal density function
\end_layout

\begin_layout Standard
Gaussian random variables are used in many continuous applications.
 It is useful because it can be manipulated easily in several situations.
 Gaussian density function is defined over a sample space that includes
 all real numbers.
 It is usually defined as
\begin_inset Formula 
\[
p(y|\mu,\sigma^{2})=\frac{1}{\sigma\sqrt{2\pi}}\exp\left\{ -\frac{1}{2\sigma^{2}}(y-\mu)^{2}\right\} 
\]

\end_inset

where the conditioning is done over two variables: the mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Gaussian density function also can be written in shorthand notation as
\begin_inset Formula 
\[
p(y|\mu,\sigma^{2})=\mathcal{N}(\mu,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
In Python Numpy we can use np.random.randn to draw sample from standard normal
 distribution, with 
\begin_inset Formula $\mu=1$
\end_inset

 and 
\begin_inset Formula $\sigma=1$
\end_inset

.
 For random samples from 
\begin_inset Formula $\mathcal{N}(\mu,\sigma^{2})$
\end_inset

 we can use
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

sigma * np.random.randn(...) + mu
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Multivariate Gaussian
\end_layout

\begin_layout Standard
The Gaussian density function also can be generalized for continous vectors.
\begin_inset Formula 
\[
p(\mathbf{x})=\frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathsf{T}}\boldsymbol{\Sigma^{-1}}(\mathbf{x}-\boldsymbol{\mu})\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
In Python, we can use numpy.random.multivariate_normal to draw samples from
 multivariate normal distribution.
\end_layout

\end_body
\end_document
