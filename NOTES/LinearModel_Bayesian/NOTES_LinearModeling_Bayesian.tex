\input{../PREAMBLE_tufte}

\begin{document}


\title{Linear Modeling - Bayesian Approach\\
TF4063}
\author{Fadjar Fathurrahman}
\date{}
\maketitle

The material in this note is based on Rogers2017.

\input{CoinBayesian}

\section{Bayesian approach to Olympic 100m data}

Let's revisit our linear model:
\begin{equation}
t_{n} = \mathbf{w}^{\mathsf{T}}\mathbf{x}_{n} + \epsilon_{n}
\end{equation}
where $\mathbf{w} = \left[ w_{0}, \ldots, w_{K} \right]^{\mathsf{T}}$
$\mathbf{x}_{n} = \left[ 1, x_{n}, x^{2}_{n}, \ldots, x_{n}^{K} \right]$.
Combining all inputs into a single matrix
$\mathbf{X} = \left[ \mathbf{x}_{1}, \ldots, \mathbf{x}_{N} \right]$
we can write:
\begin{equation}
\mathbf{t} = \mathbf{Xw} + \boldsymbol{\epsilon}
\end{equation}

As opposed to the maximum likelihood approach that we have used before,
we will now treat the model parameters, i.e. $\mathbf{w}$, as random variables.
To simplify the analysis, however, we will assume that we know the true value
of $\sigma^2$.

The quantity that will be focusing on is
\begin{equation}
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2, \Delta)
\end{equation}
which can be calculated using Bayes' rule
\begin{align}
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2, \Delta) & = 
\frac{p(\mathbf{t}|\mathbf{w},\mathbf{X},\sigma^2,\Delta) p(\mathbf{w} | \Delta)}%
{p(\mathbf{t} | \mathbf{X},\sigma^2,\Delta)} \\
& = \frac{p(\mathbf{t}|\mathbf{w},\mathbf{X},\sigma^2) p(\mathbf{w} | \Delta)}%
{p(\mathbf{t} | \mathbf{X},\sigma^2,\Delta)}
\end{align}
where $\Delta$ corresponds to some set of parameters required to define the prior
over $\mathbf{w}$ that will be defined more precisely later.

Expanding the marginal likelihood:
\begin{equation}
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2, \Delta) = 
\frac{p(\mathbf{t}|\mathbf{w},\mathbf{X},\sigma^2) p(\mathbf{w} | \Delta)}%
{\int p(\mathbf{t} | \mathbf{X},\sigma^2) p(\mathbf{w}|\Delta)\,\mathrm{d}\mathbf{w}}
\end{equation}

We are interested in making predictions which will involve taking an expectation
w.r.t this posterior density.
For a set of attributes $\mathbf{x}_{\mathrm{new}}$ corresponding to a new Olympic
year, the density over the associated winning time $t_{\mathrm{new}}$ is given
by
\begin{equation}
p(t_{\mathrm{new}} | \mathbf{x}_{\mathrm{new}},\mathbf{X}, \mathbf{t}, \sigma^2,\Delta) =
\int p(t_{\mathrm{new}} | \mathbf{x}_{\mathrm{new}}, \mathbf{w}, \sigma^2)
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2, \Delta)\,\mathrm{d}\mathbf{w}
\end{equation}

Now, let's consider the terms involved in the Bayes' rule expression.

\subsection{The likelihood}
The likelihood $p(\mathbf{t}, \mathbf{X}, \sigma^2)$ is the quantity that we
maximized previously, i.e.
\begin{equation}
p(\mathbf{t} | \mathbf{w}, \mathbf{X}, \sigma^2 ) =
\mathcal{N}( \mathbf{Xw}, \sigma^2\mathbf{I}_{N} )
\end{equation}
The likelihood is $N$-dimensional Gaussian density with mean $\mathbf{Xw}$ and
variance $\sigma^2\mathbf{I}_{N}$

\subsection{The prior}
Because we are interested in being able to produce an exact expression for the posterior,
we will choose the prior as Gaussian:
\begin{equation}
p(\mathbf{w} | \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}) = 
\mathcal{N}(\boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0})
\end{equation}
where the parameters $\Delta = \{ \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}\}$
will be chosen later.

\subsection{The posterior}
Because the likelihood and the posterior are Gaussians, the posterior will also be
a (multivariate) Gaussian.
It can be shown that the posterior is
\begin{align}
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2) & =
\mathcal{N}(\boldsymbol{\mu}_{\mathbf{w}},\boldsymbol{\Sigma}_{\mathbf{w}}) \\
& = \exp\left(
-\frac{1}{2}(\mathbf{w} - \boldsymbol{\mu}_{\mathbf{w}})^{\mathsf{T}}
\boldsymbol{\Sigma}^{-1}_{\mathbf{w}}
(\mathbf{w} - \boldsymbol{\mu}_{\mathbf{w}})
\right)
\end{align}
with covariance matrix:
\begin{equation}
\boldsymbol{\Sigma}_{\mathbf{w}} = \left(
\frac{1}{\sigma^2}\mathbf{X}^{\mathsf{T}}\mathbf{X} + \boldsymbol{\Sigma}^{-1}_{0}
\right)^{-1}
\end{equation}
and mean vector:
\begin{equation}
\boldsymbol{\mu}_{\mathbf{w}} = \boldsymbol{\Sigma}_{\mathbf{w}}
\left(
\frac{1}{\sigma^2}\mathbf{X}^{\mathsf{T}}\mathbf{t} +
\boldsymbol{\Sigma}_{0}^{-1}\boldsymbol{\mu}_{0}
\right)
\end{equation}

Given the new observation $\mathbf{x}_{\mathrm{new}}$, we can make
predicition as expectation
\begin{equation*}
p(t_{\mathbf{new}} | \mathbf{x}_{\mathrm{new}}, \mathbf{X}, \mathbf{t}, \sigma^2) =
\int p(t_{\mathbf{new}} | \mathbf{x}_{\mathrm{new}}, \mathbf{w}, \sigma^2)
p(\mathbf{w} | \mathbf{t}, \mathbf{X}, \sigma^2)\,\mathrm{d}\mathbf{w}
\end{equation*}
By our model this is defined as the product of $\mathbf{x}_{\mathrm{new}}$
and $\mathbf{w}$ with some additive Gaussian noise:
\begin{equation*}
p(t_{\mathbf{new}} | \mathbf{x}_{\mathrm{new}}, \mathbf{X}, \mathbf{t}, \sigma^2) =
\mathcal{N}(\mathbf{x}_{\mathrm{new}}^{\mathsf{T}}\mathbf{w}, \sigma^2)
\end{equation*}
Because this expression and the posterior are both Gaussian, the result of the
integral is another Gaussian. In general, if
$p(\mathbf{w}|\boldsymbol{\mu},\boldsymbol{\Sigma}) =
\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, then the expectation
is another Gaussian density
\begin{equation}
\mathcal{N}(\mathbf{x}_{\mathrm{new}}\boldsymbol{\mu}_{\mathbf{w}},
\sigma^2 +
\mathbf{x}_{\mathrm{new}}^{\mathsf{T}}\boldsymbol{\Sigma}_{\mathbf{w}}
\mathbf{x}_{\mathrm{new}})
\end{equation}

\begin{figure}[h]
{\centering
\includegraphics[width=\textwidth]{images_priv/Rogers_Fig_3_20.pdf}
\par}
\caption{Evolution of the posterior density and example functions
drawn from the posterior for the Olympic data as observations are
added.}
\label{fig:Rogers_3_20}
\end{figure}


\bibliographystyle{unsrt}
\bibliography{BIBLIO}

\end{document}
