%\documentclass[a4paper,11pt]{article} % print setting
\documentclass[a4paper,11pt]{article} % screen setting

\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=1.5cm,bmargin=1.5cm,lmargin=1.5cm,rmargin=10.0cm}

\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}

%\usepackage{cmbright}
%\renewcommand{\familydefault}{\sfdefault}

%\usepackage{fontspec}
\usepackage[libertine]{newtxmath}
\usepackage[no-math]{fontspec}
\setmainfont{Linux Libertine O}
%\setmonofont{DejaVu Sans Mono}
\setmonofont{JuliaMono-Regular}


\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}

% DARKMODE
%\pagecolor[rgb]{0,0,0} %black
%\color[rgb]{0.8,0.8,0.8} %grey

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{float}

\usepackage{minted}

\newminted{julia}{breaklines,fontsize=\footnotesize}
\newminted{python}{breaklines,fontsize=\footnotesize}

\newminted{bash}{breaklines,fontsize=\footnotesize}
\newminted{text}{breaklines,fontsize=\footnotesize}

\newcommand{\txtinline}[1]{\mintinline[breaklines,fontsize=\footnotesize]{text}{#1}}
\newcommand{\jlinline}[1]{\mintinline[breaklines,fontsize=\footnotesize]{julia}{#1}}
\newcommand{\pyinline}[1]{\mintinline[breaklines,fontsize=\footnotesize]{python}{#1}}

\newmintedfile[juliafile]{julia}{breaklines,fontsize=\footnotesize}
\newmintedfile[pythonfile]{python}{breaklines,fontsize=\footnotesize}

\definecolor{mintedbg}{rgb}{0.90,0.90,0.90}
\usepackage{mdframed}
\BeforeBeginEnvironment{minted}{
    \begin{mdframed}[backgroundcolor=mintedbg,%
        topline=false,bottomline=false,%
        leftline=false,rightline=false]
}
\AfterEndEnvironment{minted}{\end{mdframed}}


\usepackage{setspace}

\onehalfspacing

\usepackage{appendix}


\newcommand{\highlighteq}[1]{\colorbox{blue!25}{$\displaystyle#1$}}
\newcommand{\highlight}[1]{\colorbox{red!25}{#1}}



\begin{document}


\title{Linear Modeling - Maximum Likelihood\\
TF4063}
\author{Fadjar Fathurrahman}
\date{}
\maketitle

The material in this note is based on \cite{Rogers2017}.
Most of the code in this note is written in Julia programming language
\cite{Bezanson2017,juliaorg}.
We only show only some portion of the code to illustrate the idea described in the
note.

\section{Thinking generatively}
Model
\begin{equation}
t_{n} = \mathbf{w}^{\mathsf{T}}\mathbf{x}_{n} + \epsilon_{n}
\end{equation}
$\epsilon_{n}$ is a continuous random variable.
We do not just have one random variable, but one for each observed data.
We assume that these values are independent:
\begin{equation}
p(\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{N}) = \prod_{n=1}^{N} p(\epsilon_{n})
\end{equation}
We additionally assume the form pf $p(\epsilon_{n})$ is that of Gaussian
distribution with zero mean and variance $\sigma^2$.

Our model is of the following form:
\begin{equation}
t_{n} = f(\mathbf{x}_{n}; \mathbf{w}) + \epsilon_{n}, \,\, \epsilon_{n} \sim \mathcal{N}(0,\sigma^2)
\end{equation}

The loss measured the difference between the observed
values of t and those predicted by the model. The effect of adding a random variable
to the model is that the output of the model, $t$, is now itself a random variable. In
other words, there is no single value of $t_n$ for a particular $x_n$.
As such, we cannot use the loss as a means of optimizing $\mathbf{w}$
and $\sigma_{2}$

Adding a constant $\mathbf{w}^{\mathsf{T}}\mathbf{x}_{n}$ to a
Gaussian random variable is equivalent to another Gaussian random variable
with the mean shifted by the same constant:
\begin{align*}
y & = a + z \\
p(z) & = \mathcal{N}(\mu,\sigma^2) \\
p(y) & = \mathcal{N}(\mu + a,\sigma^2)
\end{align*}
Therefore, the random variable $t_n$ has the density function:
\begin{equation}
p(t_{n}|\mathbf{x},\mathbf{w},\sigma^2) =
\mathcal{N}(\mathbf{w}^{\mathsf{T}}\mathbf{x}_{n},\sigma^2)
\end{equation}


In general we are not interested in the likelihood of single data point but that of
all of the data. If we have $N$ data points we have the following joint conditional
density
\begin{equation*}
p(t_{1}, \ldots, t_{N} | \mathbf{x}_{1}, \ldots, \mathbf{x}_{N}, \mathbf{w}, \sigma^2)
\end{equation*}
This is a joint density over all of the responses in our dataset.
We will write this compactly
as $p( \mathbf{t} | \mathbf{X}, \mathbf{w}, \sigma^2)$.
Evaluating this density at the observed data points gives a single
likelihood value for the whole dataset, which we can optimise by varying $\mathbf{w}$
and $\sigma^2$.

The assumption that the noise at each data point is independent, i.e.
\begin{equation*}
p(\epsilon_{1}, \ldots, \epsilon_{N}) = \prod_{n} p(\epsilon_{n})
\end{equation*}
enables us to factorise this density into something more manageable. In
particular, this joint conditional density can be factorised into $N$ separate terms,
one for each data object:
\begin{equation}
L = p(\mathbf{t} | \mathbf{X},\mathbf{w},\sigma^2)
= \prod_{n=1}^{N} p(\mathbf{t} | \mathbf{x}_{n},\mathbf{w},\sigma^2)
= \prod_{n=1}^{N} \mathcal{N}(\mathbf{w}^{T}\mathbf{x}_{n},\sigma^2)
\end{equation}

Maximixing likelihood
\begin{align*}
\log L & = \log \prod_{n=1}^{N} \mathcal{N}(\mathbf{w}^{T}\mathbf{x}_{n},\sigma^2) \\
& = \sum_{n=1} \log \left( \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left\{ -\frac{1}{2\sigma^2}
\left( t_{n} - f(\mathbf{x};\mathbf{w}) \right)^2
\right\} \right) \\
& = \sum_{n=1}^{N} \left(
-\frac{1}{2}\log(2\pi) - \log\sigma - \frac{1}{2\sigma^2}
\left( t_{n} - f(\mathbf{x};\mathbf{w}) \right)^2 \right) \\
& = -\frac{N}{2}\log 2\pi - N\log\sigma - \frac{1}{2\sigma^2}
\sum_{n=1}^{N} \left( t_{n} - f(\mathbf{x};\mathbf{w}) \right)^2
\end{align*}

For our choice of model
$f(\mathbf{x}_{n}; \mathbf{w}) = \mathbf{w}^{\mathsf{T}}\mathbf{x}_{n}$, we have
\begin{equation}
\log L = -\frac{N}{2} \log 2\pi - N\log\sigma - \frac{1}{2\sigma^2}
\sum_{n=1}^{N} \left( t_{n} - \mathbf{w}^{\mathsf{T}}\mathbf{x}_{n} \right)^2
\end{equation}

First derivative w.r.t $\mathbf{w}$:
\begin{align*}
\frac{\partial\log L}{\partial w} & = \frac{1}{\sigma^2}
\sum_{n=1}^{N} \mathbf{x}_{n} \left( t_{n} - \mathbf{x}^{\mathsf{T}}_{n} \mathbf{w} \right) \\
& = \frac{1}{\sigma^2}\sum_{n=1}^{N} \mathbf{x}_{n} t_{n} - 
\mathbf{x}_{n}\mathbf{x}_{n}^{\mathsf{T}}\mathbf{w}
= \mathbf{0}
\end{align*}
we have used
$\mathbf{w}^{\mathsf{T}}\mathbf{x}_{n} = \mathbf{x}_{n}^{\mathsf{T}}\mathbf{w}$

We also note that ...

so we have
\begin{equation}
\frac{\partial\log L}{\partial w} = \frac{1}{\sigma^2}\left(
\mathbf{X}^{\mathsf{T}}\mathbf{t} - \mathbf{X}^{\mathsf{T}}\mathbf{X}\mathbf{w} = \mathbf{0}
\right)
\end{equation}

solving this equation we finally have
\begin{equation}
\hat{\mathbf{w}} = \left(\mathbf{X}^{\mathsf{T}}\mathbf{X}\right)^{-1}
\mathbf{X}^{\mathsf{T}}\mathbf{t}
\end{equation}

This is the maximum likelihood solution for $\mathbf{w}$ and this solution is exactly
the same as the solution obtained by minimizing the loss function.
Minimizing the squared loss is equivalent
to the maximum likelihood solution if the noise is assumed to be Gaussian.

To obtain the expression for $\sigma^2$, we can use the same procedure,
assuming that $\mathbf{w} = \hat{\mathbf{w}}$:
\begin{equation}
\frac{\partial \log L}{\partial \sigma} = -\frac{N}{\sigma} +
\frac{1}{\sigma^3}\sum_{n=1}^{N} (t_{n} - \mathbf{x}^{\mathsf{T}}\hat{\mathbf{w}})^2 = 0
\end{equation}
Rearranging the equation we have
\begin{equation}
\hat{\sigma^2} = \frac{1}{N}\sum_{n=1}^{N} (t_n - \mathbf{x}^{\mathsf{T}}\hat{\mathbf{w}})
\end{equation}
or
\begin{equation}
\hat{\sigma^2} = \frac{1}{N} = \frac{1}{N}\left(
\mathbf{t}^{\mathsf{T}}\mathbf{t} - \mathbf{t}^{\mathsf{T}}\mathbf{X}\hat{\mathbf{w}}
\right)
\end{equation}

Test program output
\begin{textcode}
w = [36.416455902505334, -0.013330885710962845]
Ïƒ2 = 0.05030711047565789
\end{textcode}

Hessian matrix:
\begin{equation}
\frac{\partial^2 \log L}{\partial\mathbf{w}\partial\mathbf{w}^{\mathsf{T}}} =
-\frac{1}{\sigma^2}\mathbf{X}^{\mathsf{T}}\mathbf{X}
\end{equation}

\begin{equation}
\frac{\partial^2\log L}{\partial \sigma^2} = \frac{N}{\sigma^2} -
\frac{3}{\sigma^4}(\mathbf{t} - \mathbf{X}\hat{\mathbf{w}})^{\mathsf{T}}
(\mathbf{t} - \mathbf{X}\hat{\mathbf{w}})
\end{equation}

The more complex model is overfitting - we have given the model too
much freedom and it is attempting to make sense out of what is essentially noise.
We showed how regularisation could be used to penalise overcomplex
parameter values. The same can be done with probabilistic models through the use
of prior distributions on the parameter values.


\bibliographystyle{unsrt}
\bibliography{BIBLIO}

\end{document}
