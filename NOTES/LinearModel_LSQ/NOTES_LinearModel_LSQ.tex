\input{PREAMBLE_tufte}

\begin{document}


\title{Linear Modeling - Least Square Approach \\
TF4063}
\author{Fadjar Fathurrahman}
\date{}
\maketitle

The material in this note is based on this book \citep{Rogers2017}.
We only show only some portion of the code to illustrate the idea described in the
note. You can find the original MATLAB codes accompanying the book at:

{\footnotesize
\url{https://github.com/sdrogers/fcmlcode}
}

\section{Linear model and its loss function}

Given pair of data $(x,t)$ where $x$ are inputs dan $t$
are targets, a linear model with parameter $(w_0, w_1)$ can
be written as:
\begin{equation}
t = f(x; w_0, w_1) = w_0 + w_1 x
\label{eq:model_linear_01}
\end{equation}

We are now left with the task of choosing the best parameters $(w_0, w_1)$
fir this model.
We need to quantify how good the model is.
One metric that we can use to quantify this is the squared difference (or
error) between target and model prediction. For $n$-th data we can write
\begin{equation}
\mathcal{L}_n \equiv \left( t_n - f(x_n; w_0, w_1) \right)^2
\end{equation}
%
By averaging contributions from all data:
\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n =
\frac{1}{N} \sum_{n=1}^{N} \left( t_n - f(x_n; w_0, w_1) \right)^2
\label{eq:loss_function_01}
\end{equation}
We will call this quantity as loss function
and we want to this quantity to be as small as possible.
Finding model parameters by minimizing loss functions such as
in Eq. \eqref{eq:loss_function_01}
is known as least square approach to linear regression.


\section{Minimizing loss function}

We can find the parameters $(w_{0},w_{1})$ by using minimization procedures:
\begin{equation}
\arg\min_{w_{0},w_{1}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_{n}
\end{equation}

For our particular case of Eq. \eqref{eq:loss_function_01}, we can found this analytically,
i.e. calculating the first derivatives of $\mathcal{L}$ with
respect to $w_0$ and $w_1$, equating
them to zero, and solve the resulting equations for $w_0$ and $w_1$.
For more general cases, we can use various numerical optimization procedures such as
gradient descent methods.

We begin by writing our loss function as:
\begin{align*}
\mathcal{L} & = \frac{1}{N} \sum_{n=1}^{N} \left( t_n - (w_0 + w_1 x_{n}) \right)^2 \\
& = \frac{1}{N} \sum_{n=1}^{N} \left( w_1^2 x_n^2 + 2w_{1}x_{n}(w_0 - t_n) + w_0^2 - 2w_0 t_n + t_n^2 \right)
\end{align*}
%
Now we find the first derivatives of $\mathcal{L}$ with respect to
$w_0$, $w_1$ and equating them to zero.
\begin{align*}
\frac{\partial\mathcal{L}}{\partial w_1} & = 2w_1 \frac{1}{N} \left( \sum_{n=1}^{N} x_n^2 \right) +
\frac{2}{N} \left( \sum_{n=1}^{N} x_{n} (w_0 - t_n) \right) = 0 \\
\frac{\partial \mathcal{L}}{\partial w_0} & = 2w_0 + 2w_1 \frac{1}{N} \left( \sum_{n=1}^{N} x_n \right) -
\frac{2}{N} \left( \sum_{n=1}^{N} t_n \right) = 0
\end{align*}

We obtain
\begin{align}
\begin{split}
w_{1} & = \frac{\overline{xt} - \overline{x}\overline{t}}{\overline{x^2} - \overline{x}^2} \\
w_{0} & = \overline{t} - w_{1} \overline{x}
\end{split}
\label{eq:w0_w1_simple}
\end{align}
where symbols with overline denotes their average value, for examples
\begin{align*}
\overline{x} & = \frac{1}{N} \sum_{n=1}^{N} x_{n} \\
\overline{t} & = \frac{1}{N} \sum_{n=1}^{N} t_{n}
\end{align*}

\textbf{Example}

Now we want to implement least square approach to linear regression based on
Eq. \ref{eq:w0_w1_simple}. For this purpose, we need a simple data to work on.
We choose to work with \txtinline{olympic100m} data which describes 
winning time of men's 100m sprint Olympic Games. You can choose to work with other
data or synthetic data.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|}
\hline
Year & Seconds \\
\hline
1896 & 12.00 \\
1900 & 11.00 \\
1904 & 11.00 \\
...  & ...   \\
2008 & 9.69  \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{pythoncode}
import numpy as np
import matplotlib.pyplot as plt
  
import matplotlib
matplotlib.style.use("seaborn-darkgrid")
  
# Load the data
DATAPATH = "olympic100m.txt"
data = np.loadtxt(DATAPATH, delimiter=",")
  
Ndata = len(data) # data.shape[0]
  
x = data[:,0]
t = data[:,1]
  
# Calculate parameters
tbar = np.sum(t)/Ndata # np.average also can be used
xbar = np.sum(x)/Ndata
xtbar = np.sum(x*t)/Ndata
x2bar = np.sum(x**2)/Ndata
  
w1 = (xtbar - xbar*tbar)/(x2bar - xbar**2)
w0 = tbar - w1*xbar
  
print("Model parameters:")
print("w0 = %18.10e" % w0)
print("w1 = %18.10e" % w1)
  
t_pred = w0 + w1*x
  
plt.clf()
plt.plot(x, t, marker="o", linewidth=0, label="data")
plt.plot(x, t_pred, marker="x", label="linear-fit")
plt.grid(True)
plt.legend()
plt.xlabel("Year")
plt.ylabel("Time (seconds)")
plt.savefig("IMG_fit_linear_olympic100_simple.pdf")  
\end{pythoncode}

We have the following result for the weights (paramaters of the model)
\begin{textcode}
Model parameters:
w0 =   3.6416455903e+01
w1 =  -1.3330885711e-02
\end{textcode}

We can visualize the result by plotting the data and the a line described
linear equation Eq. \eqref{eq:model_linear_01}. The result is shown in
Fig. \ref{fig:linreg_01}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.60]{codes_py/IMG_fit_linear_olympic100_simple.pdf}
\end{center}
\caption{Result of linear regression for \txtinline{olympic100m}.}
\label{fig:linreg_01}
\end{figure}


\section{Using matrix and vector notation}

We will rewrite our previous problem in matrix and vector notation. This will
give us more flexibility and enable us to generalize to more complex situations.
We start by defining inputs and model parameters as vectors.
\begin{equation*}
\mathbf{x}_{n} = \begin{bmatrix}
1 \\
x_{n}
\end{bmatrix}
,\,\,\,%
\mathbf{w} = \begin{bmatrix}
w_{0} \\
w_{1}
\end{bmatrix}
\end{equation*}

Using this definition, we can write our previous linear model in Eq.
\eqref{eq:model_linear_01} as:
\begin{equation}
f(x_n; w_0, w_1) = \mathbf{w}^{\mathsf{T}} \mathbf{x}_{n}
\label{eq:model_linear_02}
\end{equation}

The expression for loss function, Eq. \eqref{eq:loss_function_01}, becomes
\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{n=1}^{N} \left( t_{n} - \mathbf{w}^{\mathsf{T}}
\mathbf{x}_{n} \right)^2
\label{eq:loss_function_02}
\end{equation}
%
We now arrange several input vector into a matrix:
%
\begin{equation*}
\mathbf{X} = \begin{bmatrix}
\mathbf{x}^{\mathsf{T}}_{1} \\
\mathbf{x}^{\mathsf{T}}_{2} \\
\vdots \\
\mathbf{x}^{\mathsf{T}}_{N}
\end{bmatrix} =
\begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots \\
1 & x_{N} \\
\end{bmatrix}
\end{equation*}

As with inputs, we now define target vectors as
\begin{equation}
\mathbf{t} = \begin{bmatrix}
t_1 \\
t_2 \\
\vdots \\
t_N
\end{bmatrix}
\end{equation}
%
With this definition we can write the loss function as
\begin{equation}
\mathcal{L} = \frac{1}{N} \left( \mathbf{t} - \mathbf{Xw} \right)^{\mathsf{T}}
\left( \mathbf{t} - \mathbf{Xw} \right)
\end{equation}

To find the best value of $\mathbf{w}$ we can follow similar procedure that we have used
in the previous part. We need to find the solution of
$\dfrac{\partial \mathcal{L}}{\partial \mathbf{w}} = 0$

\begin{align}
\mathcal{L} & = \frac{1}{N} \left(
\mathbf{t}^{\mathsf{T}} \mathbf{t} +
\left(\mathbf{Xw}\right)^{\mathsf{T}} \mathbf{Xw} -
\mathbf{t}\mathbf{Xw} -
\left(\mathbf{Xw}\right)^{\mathsf{T}} \mathbf{t}
\right) \\
& = \frac{1}{N} \left(
\mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} \mathbf{X} \mathbf{w} -
2 \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{t} +
\mathbf{t}^{\mathsf{T}} \mathbf{t}
\right)
\end{align}
Equating these to zeros we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} =
\frac{2}{N} \left( \mathbf{X}^{\mathsf{T}} \mathbf{Xw} - \mathbf{X}^{\mathsf{T}}\mathbf{t} \right) = 0
\end{equation}
So we have
\begin{equation}
\mathbf{X}^{\mathsf{T}} \mathbf{Xw} = \mathbf{X}^{\mathsf{T}} \mathbf{t}
\end{equation}
or
\begin{equation}
\mathbf{w} = \left(\mathbf{X}^{\mathsf{T}}\mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{t}
\label{eq:w_vektor}
\end{equation}

The following snippet describes how Eq. \eqref{eq:w_vektor} is implemented.
\begin{pythoncode}
# Load the data
DATAPATH = "olympic100m.txt"
data = np.loadtxt(DATAPATH, delimiter=",")
  
Ndata = len(data) # data.shape[0]
  
x = data[:,0]
t = data[:,1]
  
# Build the input matrix
X = np.zeros((Ndata,2))
X[:,0] = 1.0
X[:,1] = data[:,0]
  
# Calculate the model parameters
XtX = X.T @ X
XtXinv = np.linalg.inv(XtX)
w = XtXinv @ X.T @ t
  
print("Model parameters:")
print("w0 = %18.10e" % w[0])
print("w1 = %18.10e" % w[1])
  
t_pred = X @ w

# Plot the results ...
\end{pythoncode}

The result should be the same as using Equation \eqref{eq:w0_w1_simple}:
\begin{textcode}
Model parameters:
w0 =   3.6416455903e+01
w1 =  -1.3330885711e-02  
\end{textcode}


\section{Generalization to more complex models}

We can use more "complex" models than \eqref{eq:model_linear_02}. For example
the quadratic equation:
\begin{equation}
f(x; w_{0}, w_{1}, w_{2}) = w_{0} + w_{1}x + w_{2}x^{2}
\label{eq:quadratic_eq}
\end{equation}
Note that the model is still linear in parameter, so this model is also a linear model.
Using matrix and vector notation, we can fit the data to this equation simply by
adding one column to the matrix $\mathbf{X}$:
\begin{equation}
\mathbf{X} = \begin{bmatrix}
\mathbf{x}^{\mathsf{T}}_{1} \\
\mathbf{x}^{\mathsf{T}}_{2} \\
\vdots \\
\mathbf{x}^{\mathsf{T}}_{N}
\end{bmatrix} =
\begin{bmatrix}
1 & x_{1} & x_{1}^{2} \\
1 & x_{2} & x_{2}^{2} \\
\vdots & \vdots & \vdots \\
1 & x_{N} & x_{N}^{2} \\
\end{bmatrix}
\end{equation}

The following snippets shows how to implement this:
\begin{pythoncode}
# Build matrix X
X = np.zeros( (Ndata,3) )
X[:,0] = 1.0
X[:,1] = data[:,0]
X[:,2] = np.power( data[:,0], 2 )
  
t = data[:,1] # target
  
XtX = X.transpose() @ X
XtXinv = np.linalg.inv(XtX)
w = XtXinv @ X.transpose() @ t

print("Model parameters:")
print("w0 = %18.10e" % w[0])
print("w1 = %18.10e" % w[1])
print("w2 = %18.10e" % w[2])

t_pred = X @ w
\end{pythoncode}

This scheme also applies to higher order polynomials. The following snippet shows
how to build the matrix $\mathbf{X}$, find the parameters $\mathbf{w}$ and
also how to make prediction given model parameter and input.
\begin{pythoncode}
def fit_polynomial(x, t, Npoly):
    Ndata = len(x)
    # Npoly is degree of the polynomial
    X = np.zeros( (Ndata,Npoly+1) )
    X[:,0] = 1
    for i in range(1,Npoly+1):
        X[:,i] = np.power( x, i )
    XtX = X.transpose() @ X
    XtXinv = np.linalg.inv(XtX)
    w = XtXinv @ X.transpose() @ t
    return X, w

Npoly = 3
X, w = fit_polynomial(x, t, Npoly)

# Define new input from first x to last x where the model will be evaluated
NptsPlot = 200
x_eval = np.linspace(x[0], x[-1], NptsPlot)
# Build X matrix for new input
X_eval = np.zeros( (NptsPlot,Npoly+1) )
X_eval[:,0] = 1.0
for i in range(1,Npoly+1):
    X_eval[:,i] = np.power( x_eval, i )

# Evaluate the model for the new input  
t_eval = X_eval @ w
\end{pythoncode}

In the following figure we show the result of fitting \txtinline{olympic100m} data
to 3rd order polynomial.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.60]{codes_py/IMG_fit_poly3_olympic100.pdf}
\end{center}
\caption{3rd order polynomial fit to \txtinline{olympic100m} data.}
\end{figure}

Note that we have shifted and scaled the $x$-axis (the input) to avoid
numerical problems (round-off errors) with high order polynomial.
The following snippet can be used for this purpose.
\begin{pythoncode}
# Load the data
DATAPATH = "olympic100m.txt"
data = np.loadtxt(DATAPATH, delimiter=",")

t = data[:,1] # Target
# Rescale the data to avoid numerical problems with large numbers
x = data[:,0]
x = x - x[0] # shift
x = 0.25*x # scale
\end{pythoncode}


\section{Model selection}

Evaluating the value of loss according to Eq. \eqref{eq:loss_function_02}
on the observation data for different polynomial order
we have the following result.
\begin{textcode}
Order:   1 Loss:    0.05031
Order:   2 Loss:    0.03796
Order:   3 Loss:    0.02961
Order:   4 Loss:    0.02706
Order:   5 Loss:    0.02350
Order:   6 Loss:    0.02202
Order:   7 Loss:    0.01970
Order:   8 Loss:    0.01698    
\end{textcode}
We note that the loss is decreasing as we increase the polynomial order.
Using this result, we might decide to choose 8-th order polynomial as the
best model. However, this is not a good idea.
The result of 8-th order fit is shown in Figure \ref{fig:8th_order_fit}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.60]{codes_jl/IMG_linreg_polynom_8.pdf}
\end{center}
\caption{8-th order polynomial fit to \txtinline{olympic100m} data.}
\label{fig:8th_order_fit}
\end{figure}
It can be seen that the model is not behaved very well if we try to predict
$t$ for other years other than the ones in our original data.

One solution to mitigate this problem is to split the data that we have
into two subsets: the \emph{training} and \emph{validation} data.
The model parameters $\mathbf{w}$ is calculated (or trained) using
training data while the loss is calculated using validation data.

As an example, let's say that we want to use the data for year larger
than 1979 as validation data and otherwiseas training data. The following
snippet shows how to do this.
\begin{pythoncode}
def fit_polynomial(x, t, Npoly):
    Ndata = len(x)
    # Npoly is degree of the polynomial
    X = np.zeros( (Ndata,Npoly+1) )
    X[:,0] = 1
    for i in range(1,Npoly+1):
        X[:,i] = np.power( x, i )
    XtX = X.transpose() @ X
    XtXinv = np.linalg.inv(XtX)
    w = XtXinv @ X.transpose() @ t
    return X, w
  
def predict_polynomial(w, x_eval):
    Npoly = w.shape[0] - 1
    Ndata_eval = x_eval.shape[0]
    # Build X matrix for new input
    X_eval = np.zeros( (Ndata_eval,Npoly+1) )
    X_eval[:,0] = 1.0
    for i in range(1,Npoly+1):
        X_eval[:,i] = np.power( x_eval, i )
    # evaluate
    t_eval = X_eval @ w
    return t_eval
  
# Load the data
DATAPATH = "olympic100m.txt"
data = np.loadtxt(DATAPATH, delimiter=",")
  
t_full = data[:,1] # Target
x_full = data[:,0]
# Data indices for validation and training data
idx_val = x_full > 1979
idx_train = x_full <= 1979
#
x_val = x_full[idx_val]
t_val = t_full[idx_val]
#
x = x_full[idx_train]
t = t_full[idx_train]
  
# Shift and rescale the data to avoid numerical
# problems with large numbers
x = x - x_full[0]
x = 0.25*x
# also do this for validation input
x_val = x_val - x_full[0]
x_val = 0.25*x_val
  
for Npoly in range(1,9):
    X, w = fit_polynomial(x, Npoly)
    t_val_pred = predict_polynomial(w, x_val)
    loss = np.sum( (t_val_pred - t_val)**2/len(t_val) )
    print("Npoly = %2d   loss = %10.5f" % (Npoly, loss))  
\end{pythoncode}

The result is shown below.
\begin{textcode}
Npoly =  1   loss =    0.10130
Npoly =  2   loss =    0.16763
Npoly =  3   loss =    1.06188
Npoly =  4   loss =    4.45706
Npoly =  5   loss =    5.51420
Npoly =  6   loss = 1533.43686
Npoly =  7   loss =   61.91356
Npoly =  8   loss = 6023.27314
\end{textcode}
From this result, we can say that first order polynomial is the best model.

However, note that the above result is very sensitive on the choice of validation data.
This is particularly problematic if our data is small.
A technique called \emph{cross-validation} can be used to mitigate this problem.
$K$-fold cross-validation splits the data into $K$ equally (or as close to equal as
possible) sized blocks.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=1.0]{../images_priv/Rogers_Fig_1_14.pdf}
\end{center}
\caption{Dividing data into train and validation sets for cross validation.}
\end{figure}

Each block takes its turn as a
validation set for a training set comprised of the other $K-1$ blocks. Averaging over
the resulting $k$ loss values gives us our final loss value.

An extreme case of $K$-fold
cross-validation is where $K = N$ , the number of observations in our dataset: each
data observation is held out in turn and used to test a model trained on the other
This particular form of cross-validation is also known as Leave-One-Out
Cross-Validation (LOOCV).

\begin{mdframed}[topline=false,bottomline=false,leftline=true,rightline=false]
\textbf{Task 1} \\
Implement LOOCV on \txtinline{olympic100m} dataset. Which polynomial gives the
minimum loss?
\end{mdframed}

One drawback of illustrating model selection on a real dataset is that we don't
know what the "true" model is and therefore don't know if our selection techniques
are working. We can overcome this by generating a synthetic dataset.

\begin{mdframed}[topline=false,bottomline=false,leftline=true,rightline=false]
\textbf{Task 2}\\
Create a synthetic dataset from, for example generated from 3rd order
polynomial plus random noise term. Fit polynomial from first to, say 7th order,
to this data. Plot order vs LOOCV loss. Which polynomial order give the minimum loss?
\end{mdframed}

\section{Regularization}

We can define a measure of complexity of out linear model by
\begin{equation}
\sum_{i} w_{i}^{2}\,\,\text{or }\mathbf{w}^{\mathsf{T}}\mathbf{w}
\end{equation}

As opposed to minimizing loss function \eqref{eq:loss_function_02}, we can minimize
a regularized loss function by adding penalty for overcomplexity:
\begin{equation*}
\mathcal{L}' = \mathcal{L} + \lambda \mathbf{w}^{\mathsf{T}} \mathbf{w}
\end{equation*}
where the arameter $\lambda$ control the trade off between model accuracy and
model complexity.
To find the best parameter, we can proceed by using similar procedure as before.
The regularized loss function is written as
\begin{equation}
\mathcal{L}' = \frac{1}{N} \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} \mathbf{X} \mathbf{w}
- \frac{2}{N} \mathbf{w}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} \mathbf{t}
+ \frac{1}{N} \mathbf{t}^{\mathsf{T}} \mathbf{t}
+ \lambda \mathbf{w}^{\mathsf{T}} \mathbf{w}
\end{equation}
First derivative of the loss function with respect to model paramater $\mathbf{w}$ is
\begin{equation*}
\frac{\partial \mathcal{L}'}{\partial \mathbf{w}} =
\frac{2}{N} \mathbf{X}^{\mathsf{T}} \mathbf{X} \mathbf{w}
- \frac{2}{N} \mathbf{X}^{\mathsf{T}} \mathbf{t} + 2\lambda\mathbf{w}
\end{equation*}
Equating this to zero:
\begin{equation*}
( \mathbf{X}^{\mathsf{T}} \mathbf{X} + N \lambda \mathbf{I} ) \mathbf{w} = \mathbf{X}^{\mathsf{T}} \mathbf{t}
\end{equation*}
we obtain
\begin{equation}
\mathbf{w} =
( \mathbf{X}^{\mathsf{T}} \mathbf{X} + N \lambda \mathbf{I} )^{-1}
\mathbf{X}^{\mathsf{T}} \mathbf{t}
\end{equation}

We can implement this by simple modification to our code.
\begin{pythoncode}
def fit_polynomial_ridge(x, t, Npoly, λ=0.0):
    Ndata = len(x)
    # Npoly is degree of the polynomial
    X = np.zeros( (Ndata,Npoly+1) )
    X[:,0] = 1.0
    for i in range(1,Npoly+1):
        X[:,i] = np.power( x, i )
    XtX = X.transpose() @ X + Ndata*λ*np.eye(Ndata)
    XtXinv = np.linalg.inv(XtX)
    w = XtXinv @ X.transpose() @ t
    return X, w
\end{pythoncode}

As an example we will implement this for synthetic data generated from linear
function plus noise term.
\begin{juliacode}
np.random.seed(1234)
Ntrain = 6 # training data
x = np.linspace(0.0, 1.0, Ntrain)
y = 2*x - 3
NoiseVar = 0.1
noise = math.sqrt(NoiseVar)*np.random.randn(x.shape[0])
t = y + noise # add some noise
\end{juliacode}


Fitting 5-th order polynomial to this data and using various values for regularization
parameter $\lambda$, we obtain the following result.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.60]{codes_py/IMG_reg_fit_poly5_synth.pdf}
\end{center}
\end{figure}
Best value for $\lambda$ can be obtained by using validation data as we have done before.

The following snippet can be used to generate the figure:
\begin{fullwidth}
\begin{pythoncode}
Npoly = 5
plt.clf()
plt.plot(x, t, marker="o", label="data")
x_eval = np.linspace(x[0], x[-1], 100)
for λ in [0.0, 1e-6, 1e-4, 1e-1]:
    X, w = fit_polynomial_ridge(x, t, Npoly, λ=λ)
    t_eval = predict_polynomial(w, x_eval)
    plt.plot(x_eval, t_eval, label="$\\lambda$={:8.1e}".format(λ))
plt.xlim(-0.05, 1.05)
plt.ylim(-3.8, -0.4)
plt.grid(True)
plt.legend()
\end{pythoncode}
\end{fullwidth}

\begin{mdframed}[topline=false,bottomline=false,leftline=true,rightline=false]
\textbf{Task 3}\\
Apply regularized linear regression to olympic100m data or synthetic data or other simple
dataset of your choice. Determine the best model parameters (along with regularization
parameter $\lambda$)
and describe the procedure that you have used to determine them.
\end{mdframed}

\section{Linear regression in Scikit Learn Python package}

Scikit Learn \citep{scikit-learn} provides many algorithms
for regression problems. For linear models such as discussed
previously, we can use several classes from \pyinline{linear_model}
module:
\begin{itemize}
\item ordinary linear regression
\item ridge linear regression
\end{itemize}


\bibliographystyle{unsrt}
\bibliography{BIBLIO}

\end{document}
